{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, List, Dict\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "def create_directory_structure(base_dir: str) -> None:\n",
    "    \"\"\"Creates the directory structure for train/valid/test splits.\"\"\"\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for subdir in ['images', 'labels']:\n",
    "            os.makedirs(os.path.join(base_dir, split, subdir), exist_ok=True)\n",
    "\n",
    "def split_dataset(image_ids: List[int], train_ratio: float, val_ratio: float, test_ratio: float = 0) -> Tuple[List[int], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        image_ids: List of all image IDs\n",
    "        train_ratio: Proportion of data for training (0-1)\n",
    "        val_ratio: Proportion of data for validation (0-1)\n",
    "        test_ratio: Proportion of data for testing (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of lists containing image IDs for each split\n",
    "    \"\"\"\n",
    "    if not 0.999 <= train_ratio + val_ratio + test_ratio <= 1.001:\n",
    "        raise ValueError(\"Split ratios must sum to 1\")\n",
    "    \n",
    "    shuffled_ids = image_ids.copy()\n",
    "    random.shuffle(shuffled_ids)\n",
    "    \n",
    "    total = len(shuffled_ids)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_ids = shuffled_ids[:train_end]\n",
    "    val_ids = shuffled_ids[train_end:val_end]\n",
    "    test_ids = shuffled_ids[val_end:] if test_ratio > 0 else []\n",
    "    \n",
    "    return train_ids, val_ids, test_ids\n",
    "\n",
    "def coco_to_yolo(coco_json_path: str, image_dir: str, output_dir: str, \n",
    "                 train_ratio: float = 0.8, val_ratio: float = 0.2, \n",
    "                 test_ratio: float = 0.0, seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Converts COCO annotations to YOLO format with train/valid/test splits.\n",
    "\n",
    "    Args:\n",
    "        coco_json_path: Path to the COCO JSON file\n",
    "        image_dir: Directory containing the source images\n",
    "        output_dir: Directory where the YOLO-formatted dataset will be saved\n",
    "        train_ratio: Proportion of data for training (default: 0.8)\n",
    "        val_ratio: Proportion of data for validation (default: 0.2)\n",
    "        test_ratio: Proportion of data for testing (default: 0.0)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Load COCO JSON\n",
    "    print(\"Loading COCO annotations...\")\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create category mappings\n",
    "    categories = coco_data['categories']\n",
    "    category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "    category_id_to_index = {cat['id']: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    # Create image mappings\n",
    "    images = coco_data['images']\n",
    "    image_id_to_info = {img['id']: img for img in images}\n",
    "    \n",
    "    # Group annotations by image\n",
    "    print(\"Processing annotations...\")\n",
    "    annotations = coco_data['annotations']\n",
    "    image_to_annotations = {}\n",
    "    for ann in annotations:\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in image_to_annotations:\n",
    "            image_to_annotations[image_id] = []\n",
    "        image_to_annotations[image_id].append(ann)\n",
    "    \n",
    "    # Create directory structure\n",
    "    create_directory_structure(output_dir)\n",
    "    \n",
    "    # Split dataset\n",
    "    all_image_ids = list(image_to_annotations.keys())\n",
    "    train_ids, val_ids, test_ids = split_dataset(all_image_ids, train_ratio, val_ratio, test_ratio)\n",
    "    \n",
    "    # Process each split\n",
    "    splits = {\n",
    "        'train': train_ids,\n",
    "        'valid': val_ids,\n",
    "        'test': test_ids\n",
    "    }\n",
    "    \n",
    "    missing_images = []\n",
    "    copied_images = []\n",
    "    \n",
    "    for split_name, split_ids in splits.items():\n",
    "        if not split_ids:  # Skip empty splits\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {split_name} split...\")\n",
    "        for image_id in tqdm(split_ids, desc=f\"Converting {split_name} annotations\"):\n",
    "            image_info = image_id_to_info[image_id]\n",
    "            image_width = image_info['width']\n",
    "            image_height = image_info['height']\n",
    "            image_file_name = image_info['file_name']\n",
    "            label_file_name = os.path.splitext(image_file_name)[0] + '.txt'\n",
    "            \n",
    "            # Setup paths\n",
    "            src_image_path = os.path.join(image_dir, image_file_name)\n",
    "            dst_image_path = os.path.join(output_dir, split_name, 'images', image_file_name)\n",
    "            label_file_path = os.path.join(output_dir, split_name, 'labels', label_file_name)\n",
    "            \n",
    "            # Copy image\n",
    "            if os.path.exists(src_image_path):\n",
    "                if not os.path.exists(dst_image_path):\n",
    "                    shutil.copy2(src_image_path, dst_image_path)\n",
    "                    copied_images.append(image_file_name)\n",
    "            else:\n",
    "                missing_images.append(image_file_name)\n",
    "                continue  # Skip creating labels for missing images\n",
    "            \n",
    "            # Write labels\n",
    "            with open(label_file_path, 'w') as label_file:\n",
    "                for ann in image_to_annotations[image_id]:\n",
    "                    class_index = category_id_to_index[ann['category_id']]\n",
    "                    \n",
    "                    if 'segmentation' in ann and ann['segmentation'] and ann.get('iscrowd', 0) == 0:\n",
    "                        # Process segmentation\n",
    "                        for seg in ann['segmentation']:\n",
    "                            coords = []\n",
    "                            for i in range(0, len(seg), 2):\n",
    "                                x = seg[i] / image_width\n",
    "                                y = seg[i+1] / image_height\n",
    "                                coords.extend([x, y])\n",
    "                            label_file.write(f\"{class_index} \" + \" \".join(map(str, coords)) + \"\\n\")\n",
    "                    \n",
    "                    elif 'bbox' in ann:\n",
    "                        # Process bbox\n",
    "                        x_min, y_min, width, height = ann['bbox']\n",
    "                        x_center = (x_min + width/2) / image_width\n",
    "                        y_center = (y_min + height/2) / image_height\n",
    "                        width = width / image_width\n",
    "                        height = height / image_height\n",
    "                        label_file.write(f\"{class_index} {x_center} {y_center} {width} {height}\\n\")\n",
    "    \n",
    "    # Generate data.yaml\n",
    "    data_yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "    with open(data_yaml_path, 'w') as data_yaml_file:\n",
    "        data_yaml_file.write(f\"train: ./train/images\\n\")\n",
    "        data_yaml_file.write(f\"val: ./valid/images\\n\")\n",
    "        if test_ratio > 0:\n",
    "            data_yaml_file.write(f\"test: ./test/images\\n\")\n",
    "        data_yaml_file.write(f\"\\nnc: {len(categories)}\\n\")\n",
    "        names = [cat['name'] for cat in categories]\n",
    "        data_yaml_file.write(f\"names: {names}\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nConversion completed!\")\n",
    "    print(f\"Dataset statistics:\")\n",
    "    print(f\"- Train images: {len(train_ids)}\")\n",
    "    print(f\"- Validation images: {len(val_ids)}\")\n",
    "    if test_ratio > 0:\n",
    "        print(f\"- Test images: {len(test_ids)}\")\n",
    "    print(f\"\\nProcessing summary:\")\n",
    "    print(f\"- Total images copied: {len(copied_images)}\")\n",
    "    if missing_images:\n",
    "        print(f\"- Missing images: {len(missing_images)}\")\n",
    "        print(\"  First few missing images:\", missing_images[:5])\n",
    "        print(\"  Check if the image_dir path is correct and contains all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COCO annotations...\n",
      "Processing annotations...\n",
      "\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train annotations: 100%|██████████| 217/217 [00:01<00:00, 199.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing valid split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting valid annotations: 100%|██████████| 55/55 [00:00<00:00, 191.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversion completed!\n",
      "Dataset statistics:\n",
      "- Train images: 217\n",
      "- Validation images: 55\n",
      "\n",
      "Processing summary:\n",
      "- Total images copied: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    coco_json_path = r'd:\\OneDrive - Personal\\DS\\FleetBlox\\Data\\Final\\Inspection\\Exterior\\Car Parts\\Final_Parts_Labels Corrected COCO\\annotations\\instances_default.json'\n",
    "    image_dir = r'D:\\OneDrive - Personal\\DS\\FleetBlox\\Data\\Final\\Inspection\\Exterior\\Car Parts\\Final_Parts_Labels Corrected COCO\\images'  # Directory containing the original images\n",
    "    output_dir = r'D:\\OneDrive - Personal\\DS\\FleetBlox\\Data\\Final\\Inspection\\Exterior\\Car Parts\\Final_Parts_Labels Corrected YOLO'\n",
    "    \n",
    "    # Convert with 80% train, 20% val split\n",
    "    coco_to_yolo(\n",
    "        coco_json_path=coco_json_path,\n",
    "        image_dir=image_dir,\n",
    "        output_dir=output_dir,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.0,\n",
    "        seed=101\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
